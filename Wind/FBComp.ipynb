{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BtSCYPLpB3M1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RbpeSPTsCUD7"
   },
   "outputs": [],
   "source": [
    "RE = \"Wind_Wallonie_Elia\"\n",
    "address = \"../data/\"\n",
    "\n",
    "data_train_csv1 = pd.read_csv(address+RE+'_20.csv', index_col=0)\n",
    "data_train_csv2 = pd.read_csv(address+RE+'_21.csv', index_col=0)\n",
    "data_train_csv  = pd.concat([data_train_csv1, data_train_csv2])\n",
    "data_val_csv    = pd.read_csv(address+RE+'_22.csv', index_col=0)\n",
    "data_test_csv   = pd.read_csv(address+RE+'_23.csv', index_col=0)\n",
    "\n",
    "data_price = pd.read_csv(address+'Price_Elia_Imbalance_20_23.csv', index_col=0)\n",
    "data_train_csv['Price(€)'] = data_price['Positive imbalance price'][:len(data_train_csv)]\n",
    "data_val_csv['Price(€)']   = data_price['Positive imbalance price'][len(data_train_csv):len(data_train_csv)+len(data_val_csv)]\n",
    "data_test_csv['Price(€)']  = data_price['Positive imbalance price'][len(data_train_csv)+len(data_val_csv):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OqARQEqDCsXK"
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    " \n",
    "Battery_Size = 0.15 #p.u.\n",
    "unit         = 1 #unit: 15 minute\n",
    " \n",
    "RE_Capacity1 = max(data_train_csv['Power(MW)'])\n",
    "RE_Capacity2 = max(data_val_csv['Power(MW)'])\n",
    "RE_Capacity3 = max(data_test_csv['Power(MW)'])\n",
    "max_price = max(data_price['Marginal incremental price'])\n",
    " \n",
    "size_train0 = int(len(data_train_csv)/unit)\n",
    "size_val0   = int(len(data_val_csv)/unit)\n",
    "size_test0  = int(len(data_test_csv)/unit)\n",
    " \n",
    "data_train0 = []; data_train = []; price_train0 = []; price_train = [];\n",
    "for i in range(size_train0):\n",
    "    data_train0  += [round(pd.Series.mean(data_train_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity1, 3)]\n",
    "    price_train0 += [round(pd.Series.mean(data_train_csv['Price(€)'][i*unit:(i+1)*unit])/max_price, 3)]\n",
    "    if data_train0[i] > 0: data_train += [data_train0[i]]; price_train += [price_train0[i]]\n",
    " \n",
    "data_val0 = []; data_val = []; price_val0 = []; price_val = []\n",
    "for i in range(size_val0):\n",
    "    data_val0  += [round(pd.Series.mean(data_val_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity2, 3)]\n",
    "    price_val0 += [round(pd.Series.mean(data_val_csv['Price(€)'][i*unit:(i+1)*unit])/max_price, 3)]\n",
    "    if data_val0[i] > 0: data_val += [data_val0[i]]; price_val += [price_val0[i]]\n",
    " \n",
    "data_test0 = []; data_test = []; price_test0 = []; price_test = []\n",
    "for i in range(size_test0):\n",
    "    data_test0  += [round(pd.Series.mean(data_test_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity3, 3)]\n",
    "    price_test0 += [round(pd.Series.mean(data_test_csv['Price(€)'][i*unit:(i+1)*unit])/max_price, 3)]\n",
    "    if data_test0[i] > 0: data_test += [data_test0[i]]; price_test += [price_test0[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "z6GhbTTLCvr4"
   },
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "n_layers       = 2\n",
    "in_size        = 1\n",
    "hidden_size    = 64\n",
    "out_size       = 1\n",
    "batch_size     = 128\n",
    "learning_rate  = 0.001\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.fc_in  = nn.Linear(in_size, hidden_size)\n",
    "        self.rnn    = nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, out_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = F.relu(self.fc_in(x))\n",
    "        x = x.view(1, -1, hidden_size)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc_out(x)\n",
    "        out = F.relu(out.view(-1, out_size))\n",
    "        return out, hidden\n",
    "        \n",
    "def train_net(model, batch, optimizer):\n",
    "    x, h, y = batch[0], batch[1], batch[2]\n",
    "    loss = F.mse_loss(model.forward(x, h)[0], y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gr_RiEWYC2hc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "MAE_train: 22.57%        MAE_val: 20.06%          MAE_test: 23.0%          \n",
      "------------------------------------------------------------------------------------------\n",
      "epoch: 2\n",
      "MAE_train: 22.57%        MAE_val: 20.06%          MAE_test: 23.0%          \n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(train_output[batch_size\u001b[38;5;241m*\u001b[39mi:batch_size\u001b[38;5;241m*\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m),:] ,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m     39\u001b[0m batch \u001b[38;5;241m=\u001b[39m [batch_x, hidden, batch_y]\n\u001b[1;32m---> 40\u001b[0m \u001b[43mtrain_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m _, hidden \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(batch_x, hidden)\n\u001b[0;32m     42\u001b[0m hidden \u001b[38;5;241m=\u001b[39m (hidden[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach(), hidden[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach())\n",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m, in \u001b[0;36mtrain_net\u001b[1;34m(model, batch, optimizer)\u001b[0m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(model\u001b[38;5;241m.\u001b[39mforward(x, h)[\u001b[38;5;241m0\u001b[39m], y)\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training LSTM\n",
    "\n",
    "total_epoch    = 100\n",
    "print_interval = 1\n",
    "\n",
    "model = LSTM()\n",
    "size_train = len(data_train)\n",
    "size_val = len(data_val)\n",
    "size_test = len(data_test)\n",
    "\n",
    "train_input = np.zeros((size_train-1, 1))\n",
    "train_output = np.zeros((size_train-1, 1))\n",
    "for i in range(size_train-1):\n",
    "    train_input[i,:] = data_train[i]\n",
    "    train_output[i,:] = data_train[i+1]\n",
    "\n",
    "val_input = np.zeros((size_val-1, 1))\n",
    "val_output = np.zeros((size_val-1, 1))\n",
    "for i in range(size_val-1):\n",
    "    val_input[i,:] = data_val[i]\n",
    "    val_output[i,:] = data_val[i+1]\n",
    "\n",
    "test_input = np.zeros((size_test-1, 1))\n",
    "test_output = np.zeros((size_test-1, 1))\n",
    "for i in range(size_test-1):\n",
    "    test_input[i,:] = data_test[i]\n",
    "    test_output[i,:] = data_test[i+1]\n",
    "\n",
    "total_batch = int((size_train-1)/batch_size) + 1\n",
    "pred_train, pred_val, pred_test = [], [], [] # Predicted Value\n",
    "mae_train,  mae_val,  mae_test  = [], [], [] # Mean Absolute Error\n",
    "\n",
    "hidden = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_x = torch.tensor(train_input[batch_size*i:batch_size*(i+1),:] ,dtype=torch.float)\n",
    "        batch_y = torch.tensor(train_output[batch_size*i:batch_size*(i+1),:] ,dtype=torch.float)\n",
    "        batch = [batch_x, hidden, batch_y]\n",
    "        train_net(model, batch, optimizer)\n",
    "        _, hidden = model.forward(batch_x, hidden)\n",
    "        hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "\n",
    "    hidden = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
    "    if epoch == 0 or (epoch+1) % print_interval == 0:\n",
    "        train_predict = model.forward(torch.tensor(train_input, dtype=torch.float), hidden)[0].detach().numpy()\n",
    "        pred_train += [list(train_predict.flatten())]\n",
    "        mae_train  += [list(np.abs(train_predict - train_output).flatten())]\n",
    "        \n",
    "        val_predict = model.forward(torch.tensor(val_input, dtype=torch.float), hidden)[0].detach().numpy()\n",
    "        pred_val += [list(val_predict.flatten())]\n",
    "        mae_val  += [list(np.abs(val_predict - val_output).flatten())]\n",
    "        \n",
    "        test_predict = model.forward(torch.tensor(test_input, dtype=torch.float), hidden)[0].detach().numpy()\n",
    "        pred_test += [list(test_predict.flatten())]\n",
    "        mae_test  += [list(np.abs(test_predict - test_output).flatten())]\n",
    "\n",
    "        MAE_train = round(100*np.mean(mae_train[-1]),2)\n",
    "        MAE_val   = round(100*np.mean(mae_val[-1]),2)\n",
    "        MAE_test  = round(100*np.mean(mae_test[-1]),2)\n",
    "\n",
    "        print(\"epoch: {}\".format(epoch+1))\n",
    "        print(\"MAE_train: {}%\".format(MAE_train).ljust(25), end=\"\")\n",
    "        print(\"MAE_val: {}%\".format(MAE_val).ljust(25), end=\"\")\n",
    "        print(\"MAE_test: {}%\".format(MAE_test).ljust(25))\n",
    "        print(\"------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoOXTK_TDNmh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE_test: 1.83%\n",
      "MBE_test: 1.48%\n",
      "REV_test: $142.547\n"
     ]
    }
   ],
   "source": [
    "# Environment\n",
    "\n",
    "E_max   = Battery_Size\n",
    "P_max   = E_max\n",
    "tdelta  = unit/4\n",
    "soc_min = 0.1\n",
    "soc_max = 0.9\n",
    "a0 = -1.031; a1 = 35; a2 = 3.685; a3 = 0.2156; a4 = 0.1178; a5 = 0.3201\n",
    "b0 = 0.1463; b1 = 30.27; b2 = 0.1037; b3 = 0.0584; b4 = 0.1747; b5 = 0.1288\n",
    "c0 = 0.1063; c1 = 62.49; c2 = 0.0437; d0 = 0.0712; d1 = 61.4; d2 = 0.0288\n",
    "N = 130*215*E_max/0.1\n",
    "beta = 10/max_price\n",
    " \n",
    "select_num = np.argmin(np.mean(mae_val,axis=1))\n",
    "select_train = np.array(pred_train[select_num][:])\n",
    "select_val = np.array(pred_val[select_num][:])\n",
    "select_test = np.array(pred_test[select_num][:])\n",
    "select_test_real = np.array(data_test[1:])\n",
    "select_test_price = np.array(price_test[1:])\n",
    " \n",
    "E = E_max/2\n",
    "mbe = []\n",
    "reward = []\n",
    "info = []\n",
    "for i in range(len(select_test)):\n",
    "    bid = select_test[i]\n",
    "    gen = select_test_real[i]\n",
    "    rat = 1\n",
    "    imb = select_test_price[i]\n",
    "    \n",
    "    soc = E/E_max\n",
    "    Voc = a0*np.exp(-a1*soc) + a2 + a3*soc - a4*soc**2 + a5*soc**3\n",
    "    Rs  = b0*np.exp(-b1*soc) + b2 + b3*soc - b4*soc**2 + b5*soc**3\n",
    "    Rts = c0*np.exp(-c1*soc) + c2\n",
    "    Rtl = d0*np.exp(-d1*soc) + d2\n",
    "    R   = Rs + Rts + Rtl\n",
    " \n",
    "    I_cmax = 1000000*E_max*(soc_max - soc)/N/(Voc*tdelta)\n",
    "    I_dmax = 1000000*E_max*(soc - soc_min)/N/(Voc*tdelta)\n",
    "    p_cmax = N*(Voc*I_cmax + I_cmax**2*R)\n",
    "    p_dmax = N*(Voc*I_dmax - I_dmax**2*R)\n",
    " \n",
    "    P_cmax = p_cmax/1000000; P_dmax = p_dmax/1000000\n",
    "    P_c = min(max(rat*(gen-bid), 0), P_max, P_cmax)\n",
    "    P_d = min(max(rat*(bid-gen), 0), P_max, P_dmax)\n",
    "    p_c = 1000000*P_c/N; p_d = 1000000*P_d/N\n",
    " \n",
    "    I_c = -(Voc - np.sqrt(Voc**2 + 4*R*p_c))/(2*R)\n",
    "    I_d = (Voc - np.sqrt(Voc**2 - 4*R*p_d))/(2*R)\n",
    "    if not np.isclose(p_c, 0):\n",
    "        eff_c = (Voc*I_c)/p_c\n",
    "        E = E + eff_c*P_c*tdelta\n",
    "        disp = gen - P_c\n",
    "        info += [[gen, round(bid,4), 'C', round(P_c,4), round(disp,4), round(eff_c,4), round(E,4)]]\n",
    "    elif not np.isclose(p_d, 0):\n",
    "        eff_d = p_d/(Voc*I_d)\n",
    "        E = E - (1/eff_d)*P_d*tdelta\n",
    "        disp = gen + P_d\n",
    "        info += [[gen, round(bid,4), 'D', round(P_d,4), round(disp,4), round(eff_d,4), round(E,4)]]\n",
    "    else:\n",
    "        disp = gen\n",
    "        info += [[gen, round(bid,4), 'N', 'N', round(disp,4), 'N', round(E,4)]]\n",
    "    \n",
    "    mbe += [abs(bid - disp)]\n",
    "    reward += [(imb*disp - imb*abs(bid-disp) - beta*(P_c+P_d))*tdelta]\n",
    " \n",
    "MAE_test = round(100*np.mean(np.abs(select_test_real - select_test)),2)\n",
    "MBE_test = round(100*np.mean(mbe),2)\n",
    "print(\"MAE_test: {}%\".format(MAE_test))\n",
    "print(\"MBE_test: {}%\".format(MBE_test))\n",
    "print(\"REV_test: ${}\".format(round(max_price*RE_Capacity3*np.mean(reward),3)))\n",
    "\n",
    "pd.DataFrame(select_train).to_csv(\"./Results/\"+RE+\"_Model2_train.csv\")\n",
    "pd.DataFrame(select_val).to_csv(\"./Results/\"+RE+\"_Model2_val.csv\")\n",
    "pd.DataFrame(select_test).to_csv(\"./Results/\"+RE+\"_Model2_FBcomp.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOsi1LCoLHThfDO+MT4aGck",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
